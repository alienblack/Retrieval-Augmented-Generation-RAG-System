# RAG with LLAMA 2.0, Langchain, and ChromaDB

This project implements a Retrieval-Augmented Generation (RAG) system using LLAMA 2.0, Langchain, and ChromaDB. The system processes a document and generates text based on queries by leveraging both retrieval and generation techniques.
## Project Overview
### Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) combines retrieval and generation to improve the quality and relevance of generated text. The RAG framework first retrieves relevant documents from a knowledge base and then generates responses conditioned on the retrieved documents. This approach is particularly useful in scenarios where up-to-date or domain-specific information is crucial.

### LLAMA 2.0

LLAMA 2.0 is a state-of-the-art language model developed by Facebook AI Research (FAIR). It is designed to perform a variety of natural language processing tasks, including text generation, classification, and question answering. In this project, we use the 7B parameter variant of LLAMA 2.0 to generate high-quality text based on retrieved documents.
Langchain

Langchain is a framework that provides tools for building applications that use large language models (LLMs). It simplifies the integration of LLMs into various applications by providing utilities for managing models, handling inputs and outputs, and optimizing performance. Langchain helps streamline the process of developing and deploying language model-based applications.

### ChromaDB

ChromaDB is a highly efficient and scalable database designed for storing and querying large collections of documents. It supports advanced retrieval mechanisms that allow for fast and accurate document retrieval based on various criteria. In this project, ChromaDB is used to store the document and enable efficient retrieval of relevant information.
Project Details
Document Used for RAG

The document used in this project for retrieval-augmented generation is:

    Preliminary Agreement Chips and Science Act Award

Model Used

The LLAMA model used in this project is:

    LLAMA 2.0 7B-Chat HF

How It Works
```
    Document Storage: The document is stored in ChromaDB for efficient retrieval.
    Query Processing: When a query is received, ChromaDB retrieves relevant parts of the document.
    Text Generation: The retrieved information is passed to LLAMA 2.0 via Langchain, which generates a response based on the query and retrieved document fragments.
    Output: The generated text is provided as the final response to the query.
```
